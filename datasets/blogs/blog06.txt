Title: Why is My Failover… Failing?
Author: JV Roig

Database failover failures are the worst. You spent all that time designing resilience and high availability for your database infra, and then when a disaster happened, somehow your users still had a bad time for some reason.
Welcome to failing failovers. Here are a few tips to make sure your failovers don't fail.
High app-side DNS time-to-live (TTL)
Scenario: You're using RDS Multi-AZ exactly because you want that sweet, sweet automatic failover. An outage happened in the primary AZ of your database, and although the failover happened as fast as expected (about a couple of minutes), your users still complained of connection failures way beyond that. What's happening?
Possible culprit: Too high DNS time-to-live settings in your applications. They are, effectively, reaching out to the old (stale) IP address of your RDS endpoint. Although Multi-AZ hostname endpoints remain the same from Primary to Standby during failover, the instance IP address can change.
If you are experiencing this sort of problem, where your RDS logs show that failovers are successful and fast but customers experience extended connectivity problems, your apps are probably caching DNS data too long. Lower TTL to improve your failover experience.
Promoted instance is under-provisioned
Scenario: You have a database instance with a few read replicas, each one dedicated to different business units in your org. In general, all the read replicas are smaller than the Primary. Because read replicas don't need to be the same size as each other, you've right-sized them to fit their specific needs - the read replica you made for Accounting, for example, is smaller than the one you made for Sales, because the Sales Department users generate a heavier load. Now, when a disaster happened and you manually promoted one of the read replicas to be the Primary, the promotion succeeded but the customers ended up complaining about performance so slow that they could barely get work done.
Possible culprit: The read replica you promoted was much smaller than the Primary, and was severely underpowered. Although you succeeded in making sure the database was available within a short time when the disaster happened, the resulting performance degradation was too much for your customers to bear.
To avoid this, you should specifically promote the largest of your read replicas. If at all possible, you should have a read replica that is as large as your Primary, and tag that as your preferred replica for promotion. Waiting for a promoted instance to be resized will only increase the wait time for your users, so making sure there's a replica that's fully ready to take over as Primary is a good step. It's unnecessary to have ALL read replicas be as large as your Primary if the volume of their handled load doesn't justify it, so don't overdo this.
Cold database cache
Scenario: You have automatic failover configured for your Primary instance, and you don't have the sizing problems described in the previous section, so you expect excellent performance after a failover. This is foolproof, you think. But then, after a failover, you still received complaints from your customers about the system feeling very sluggish. If the new Primary has the same CPU, memory and storage as the old Primary, what's causing this?
Possible culprit: Database performance isn't just about the underlying CPU, memory, or disk I/O. A huge percentage of performance is due to aggressive caching or buffering done by the database - for example, a query cache that stores the result of the most frequent queries so that they don't actually have to be executed again by the database engine, or a buffer cache containing most frequently accessed data and indexes. When a typical failover happens and an instance is promoted to be the new Primary, it doesn't have this warm cache that existed in the old Primary. Although it has the exact same data records as the Primary, it doesn't have the same performance as the old Primary, because this new Primary is starting with a cold cache, and thus relies on slow disk access (instead of fast RAM) to serve every request.
If this is a big enough problem for you that you want to make sure it is minimized, Aurora PostgreSQL has a cluster cache management (CCM) feature that can help you. Essentially, you identify a specific replica within your Aurora cluster as the failover target, and CCM ensures that the chosen failover target instance not only replicates the database records, but also the Primary's buffer cache. When the failover target actually gets promoted to Primary, it starts out with a warm cache, thus preserving performance.
Wrap up and final bonus tip!
There you go, a few tips to improve the failover experience of your critical database infrastructure.
As a final (and extremely important) tip: Test your database failover!
Only actually testing your failover process will uncover weird or unexpected issues like the ones we've discussed. Don't let an actual disaster be your only teacher.