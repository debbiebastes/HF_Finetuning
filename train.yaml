model:
  name: 'hf/qwen1.5-0.5B-chat'
  type: 'CausalLM'  # Defaults to "CausalLM"
  class: ''  # If supplied, supercedes model.type

output:
  # The finetuned model name will be the base model name plus a suffix
  suffix: '-FT501'  # Suffix added to the name of the finetuned model. If empty, this will be '-FT00'

tokenizer:
  class: ''  # If supplied, uses this instead of AutoTokenizer
  add_pad_token: true  # If True, will add pad_token as the tokenizer's pad token
  pad_token: 'eos_token'  # "eos_token" will mean tokenizer.eos_token. Anything else will be taken literally.
  padding_side: 'right'  # Either "right" or "left"

dataset:
  # For now, only local datasets inside the "datasets" folder are used
  type: 'json'
  train: 'Senti_v3/sentiv3B_set2_train.jsonl'
  eval: 'Senti_v3/sentiv3B_set2_eval.jsonl'
  prompt_template: 'prompt_templates/template_legacy.json'
  prompt_max_len: 512
  completion_max_len: 128 #For CausalLM, this is ignored
  # Future, not yet implemented:
  use_hf_datasets: false
  hf_dataset_name: ''
  hf_splits: []


from_pretrained:
  torch_dtype: ''  # Default: auto

lora:
  use_lora: false  # Set to True to create a LoRA or QLoRA adapter
  r: 8
  alpha: 32
  dropout: 0.05
  target_modules:
    - 'q'
    - 'v'
    - 'k'
    - 'o'
    - 'wi_0'
    - 'wi_1'
    - 'wo'
  # target_modules:
  #     - 'q_proj'
  #     - 'o_proj'
  #     - 'k_proj'
  #     - 'v_proj'
  #     - 'gate_proj'
  #     - 'up_proj'
  #     - 'down_proj'
  bias: 'none'
  task_type: 'CAUSAL_LM'

# Quantization Settings section
quant:
  quantize: false  # Set to True to train a quantized model (must use LoRA)
  load_in_4bit: true
  bnb_4bit_quant_type: 'nf4'
  bnb_4bit_use_double_quant: true
  bnb_4bit_compute_dtype: 'bf16'

# TrainingArguments section
train_args:
  num_epochs: 4  # Int > 0, must be specified.
  load_best_model_at_end: false  # Whether to load the best model at the end of training
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  warmup_steps: 100
  save_steps: 500000
  weight_decay: 0.01
  learning_rate: 0.0001
  logging_steps: 10
  gradient_checkpointing: true
  optim: 'adafactor'
  evaluation_strategy: 'epoch'
  save_strategy: 'steps'
  logging_strategy: 'epoch'
  log_level: 'warning'
